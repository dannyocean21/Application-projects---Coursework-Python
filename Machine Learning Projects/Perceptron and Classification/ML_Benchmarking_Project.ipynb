{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cc3e2c-581b-412b-ac4e-10a0247a802d",
   "metadata": {},
   "source": [
    "## Question 1 (50 pts)\n",
    "\n",
    "Using the perceptron learning algorithm, train a perceptron for the binary classification problem defined by the given dataset.  \n",
    "Use the following requirements:\n",
    "\n",
    "- Activation function: **Sigmoid**\n",
    "- Initial weights: **w1 = 0.5**, **w2 = –0.3**\n",
    "- Initial bias: **0.2**\n",
    "- Learning rate: **0.1**\n",
    "- Number of iterations: **100**\n",
    "\n",
    "Train the perceptron and report the final weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35602f-fb05-4a4e-9762-c57263654641",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a854c6c8-613e-488f-bf2b-0e005534414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights: [0.06235377 1.20316401]\n",
      "Final Bias: -5.05487101516288\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "X = np.array([[2.5, 2.3],\n",
    "              [1.3, 1.9],\n",
    "              [3.1, 2.8],\n",
    "              [6.5, 7.2],\n",
    "              [7.1, 6.8],\n",
    "              [8.2, 7.5]])\n",
    "\n",
    "y = np.array([0, 0, 0, 1, 1, 1])  # Target values\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.array([0.5, -0.3])   # Weights\n",
    "bias = 0.2                   # Bias term\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "# Sigmoid activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training the perceptron\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        z = np.dot(w, X[i]) + bias\n",
    "        y_pred = sigmoid(z)\n",
    "        error = y[i] - y_pred\n",
    "\n",
    "        # Update rule\n",
    "        w += learning_rate * error * X[i]\n",
    "        bias += learning_rate * error\n",
    "\n",
    "print(\"Final Weights:\", w)\n",
    "print(\"Final Bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0de99-655e-4702-b2df-2022b06e8711",
   "metadata": {},
   "source": [
    "### Final Result\n",
    "\n",
    "The perceptron converged to the following parameters:\n",
    "\n",
    "- **Final Weights:** `[0.06235377 1.20316401]`  \n",
    "- **Final Bias:** `-5.05487101516288`\n",
    "\n",
    "These learned parameters represent the decision boundary separating the two classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae80bb-cedb-4429-9d91-2e8a54ef46b5",
   "metadata": {},
   "source": [
    "## Question 2 (50 pts)\n",
    "\n",
    "Using the dataset **sank.csv**, compare the performance of the following classifiers:\n",
    "\n",
    "- Logistic Regression  \n",
    "- Naive Bayes (Gaussian)  \n",
    "- Naive Bayes (Multinomial)  \n",
    "- Decision Tree Classifier  \n",
    "\n",
    "You must evaluate them using:\n",
    "- **Accuracy**\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1-score**\n",
    "\n",
    "Finally, choose the best algorithm and explain your reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711248c-94ef-47c6-8850-40f455e84f9c",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26fc2c23-65b8-41d4-83ed-820aa13bad1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.768116</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.741259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes (Gaussian)</th>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.711409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes (Multinomial)</th>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.748603</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.697987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Accuracy  Precision    Recall  F1-score\n",
       "Logistic Regression        0.793296   0.768116  0.716216  0.741259\n",
       "Naive Bayes (Gaussian)     0.759777   0.706667  0.716216  0.711409\n",
       "Naive Bayes (Multinomial)  0.709497   0.866667  0.351351  0.500000\n",
       "Decision Tree              0.748603   0.693333  0.702703  0.697987"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"sank.csv\")\n",
    "\n",
    "# Fix missing values\n",
    "data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].median())\n",
    "\n",
    "# Label encoding for categorical columns\n",
    "encoder = LabelEncoder()\n",
    "data[\"Sex\"] = encoder.fit_transform(data[\"Sex\"])\n",
    "\n",
    "# Features & target\n",
    "X = data[[\"Class\", \"Sex\", \"Age\", \"Fare\"]]\n",
    "y = data[\"Alive\"]\n",
    "\n",
    "# Scaling (needed for logistic regression & Gaussian NB)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training/testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Naive Bayes (Gaussian)\": GaussianNB(),\n",
    "    \"Naive Bayes (Multinomial)\": MultinomialNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    # Multinomial NB requires non-negative values\n",
    "    if name == \"Naive Bayes (Multinomial)\":\n",
    "        model.fit(X_train - X_train.min(), y_train)\n",
    "        y_pred = model.predict(X_test - X_train.min())\n",
    "\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-score\": f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5279c-3def-4f3b-9c36-e82b42c164dc",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "### **Best Model: Logistic Regression**\n",
    "\n",
    "Logistic Regression performs the best overall with:\n",
    "- **Highest accuracy (79.3%)**\n",
    "- **Highest F1-score (74.1%)**\n",
    "- Balanced precision and recall\n",
    "\n",
    "This makes it the most reliable model for this classification task.\n",
    "\n",
    "### Naive Bayes (Gaussian)\n",
    "- Performs reasonably well (Accuracy: 75.9%).\n",
    "- Slightly weaker F1-score than Logistic Regression.\n",
    "\n",
    "Good, but not the top performer.\n",
    "\n",
    "### Naive Bayes (Multinomial)\n",
    "- High precision (86.7%) but **very low recall (35.1%)**.\n",
    "- Misses many positive cases → **not suitable** for this dataset.\n",
    "\n",
    "Multinomial NB is meant for **text frequency data**, not continuous numeric variables (Age, Fare).\n",
    "\n",
    "### Decision Tree\n",
    "- Reasonable but slightly unstable (Accuracy: 74.9%).\n",
    "- Potential overfitting without hyperparameter tuning.\n",
    "\n",
    "Could improve with pruning or tuned parameters.\n",
    "\n",
    "## **Final Recommendation**\n",
    "\n",
    "Use **Logistic Regression** for production or reporting,  \n",
    "and **avoid Multinomial Naive Bayes** for mixed numerical/categorical data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
