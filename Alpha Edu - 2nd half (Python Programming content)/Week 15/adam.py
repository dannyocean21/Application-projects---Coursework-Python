"""
Adam Optimizer - Теоретическое объяснение
========================================

**Adam (Adaptive Moment Estimation)** — это один из самых популярных алгоритмов оптимизации,
используемых для обучения нейронных сетей. Adam сочетает преимущества двух других алгоритмов:
Momentum и RMSProp.

Основные аспекты Adam:

1. **Цель**:
   - Оптимизировать веса модели, минимизируя функцию потерь.
   - Эффективно работает с большими данными и высокоразмерными пространствами параметров.

2. **Принципы работы**:
   Adam отслеживает два момента (в математическом смысле) градиентов:
   - **m** — Экспоненциально взвешенное среднее градиентов (первый момент).
   - **v** — Экспоненциально взвешенное среднее квадрата градиентов (второй момент).

3. **Шаги алгоритма**:
   - Инициализация параметров: 
     - `m` (первый момент) и `v` (второй момент) устанавливаются в ноль.
   - Для каждого параметра модели обновление выполняется следующим образом:
     1. Вычисляется градиент функции потерь по параметру `θ` (текущий вес модели).
     2. Обновляются моменты:
        - `m_t = β1 * m_t-1 + (1 - β1) * g_t`
        - `v_t = β2 * v_t-1 + (1 - β2) * (g_t^2)`
        Где:
        - `g_t` — текущий градиент.
        - `β1` и `β2` — гиперпараметры (обычно β1=0.9, β2=0.999).
     3. Выполняется коррекция смещения:
        - `m_t_corrected = m_t / (1 - β1^t)`
        - `v_t_corrected = v_t / (1 - β2^t)`
     4. Обновляются параметры модели:
        - `θ_t = θ_t-1 - α * m_t_corrected / (sqrt(v_t_corrected) + ε)`
        Где:
        - `α` — скорость обучения (learning rate).
        - `ε` — небольшое значение для предотвращения деления на ноль (обычно 10^-8).

4. **Преимущества Adam**:
   - Быстро сходится.
   - Подстраивает скорость обучения для каждого параметра.
   - Объединяет методы Momentum и RMSProp для лучшей устойчивости.

5. **Недостатки**:
   - Может не сходиться в задачах с редкими градиентами.
   - Рекомендуется проверять и настраивать гиперпараметры.

Теперь посмотрим, как Adam реализован в PyTorch.
"""