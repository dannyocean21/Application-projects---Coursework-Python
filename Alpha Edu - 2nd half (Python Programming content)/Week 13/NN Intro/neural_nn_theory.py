

#* Файл с теоретическими объяснениями ключевых процессов нейронной сети для классификации MNIST

#? === Общая теория о нейронной сети для классификации ===
# Нейронная сеть — это математическая модель, которая имитирует работу человеческого мозга для решения задач, таких как классификация изображений.
# Мы обучаем сеть на данных MNIST (рукописные цифры), чтобы она могла различать цифры от 0 до 9. 

#! === Основные этапы работы модели ===
#? 1. **Предобработка данных:**
#    - Входные данные (изображения 28x28 пикселей) преобразуются в числовой вид.
#    - Каждое изображение нормализуется для уменьшения разброса значений пикселей и ускорения обучения.
#
#? 2. **Архитектура модели:**
#    - Модель состоит из слоев (входного, скрытых и выходного), которые связаны друг с другом.
#    - Каждый слой отвечает за определённый уровень обработки данных:
#      * Входной слой принимает необработанные данные.
#      * Скрытые слои извлекают важные признаки (например, формы и контуры цифр).
#      * Выходной слой возвращает вероятности для каждого из 10 классов (цифры от 0 до 9).
#
#? 3. **Прямое распространение (forward pass):**
#!    - Данные проходят через слои сети.
#!    - В каждом слое происходит умножение входных данных на ВЕСА, добавление смещения (!ТУДУ: bias) и применение функции АКТИВАЦИИИИИИ (например, ReLU).
#!    - Результат передаётся следующему слою, пока не достигнет выхода модели (логитов).
#
#? 4. **Функция потерь (loss function):**
#!    - Сравнивает предсказания модели с истинными метками классов (10 в нашем случае).
#!    - В нашем случае используется CrossEntropyLoss, которая включает:
#      * Преобразование логитов в вероятности (softmax).
#      * Вычисление ошибки как отрицательного логарифма вероятности правильного класса.
#
#? 5. **Обратное распространение (backward pass backpropagation):**
#!    - С помощью градиентов, рассчитанных на основе ошибки, сеть обновляет свои параметры (веса и смещения).
#!    - Градиенты вычисляются для каждого параметра, чтобы понять, как его изменение влияет на ошибку модели.
#
#! 6. **Оптимизация (optimization):**
#!    - Оптимизатор (например, SGD) обновляет параметры модели с учётом градиентов и скорости обучения (learning rate).
#!    - Цель оптимизатора — минимизировать функцию потерь, постепенно улучшая качество предсказаний.
#
#! 7. **Процесс обучения:**
#!    - Сеть обучается на тренировочных данных, проходя через множество эпох epochs (повторения).
#!    - В каждой эпохе модель проходит все данные, корректируя параметры после каждого батча (списка данных).
#
#! 8. **Оценка модели:**
#!    - После обучения точность модели проверяется на тестовом наборе данных.
#!    - Модель предсказывает классы для тестовых изображений, и эти предсказания сравниваются с истинными метками.

# === Что делает модель от 0 до идеала ===
# 1. **Инициализация:**
#    - Параметры сети (веса и смещения) инициализируются случайными значениями.
#    - На этом этапе сеть предсказывает случайные результаты.
#
# 2. **Первый проход данных:**
#    - Данные проходят через слои, создавая первые предсказания.
#    - Ошибка высокая, так как веса ещё не адаптированы к данным.
#
# 3. **Первые обновления:**
#    - Градиенты вычисляются, показывая, как параметры должны измениться для уменьшения ошибки.
#    - Оптимизатор корректирует веса и смещения, делая модель немного лучше.
#
# 4. **Постепенное улучшение:**
#    - На каждой эпохе сеть становится более точной, извлекая всё больше признаков из данных.
#    - Ошибка на тренировочном наборе уменьшается.
#
# 5. **Обученная модель:**
#    - После нескольких эпох сеть достигает хорошей точности на тренировочных данных.
#    - Однако важно убедиться, что она хорошо работает и на тестовых данных (проверка на переобучение).

# === Важные моменты ===
# - **Почему используются скрытые слои?**
#   Скрытые слои позволяют модели извлекать сложные признаки из данных, такие как углы, линии и формы.
#
# - **Почему нужна функция активации?**
#   Без неё сеть будет линейной и не сможет решать сложные задачи. ReLU ускоряет обучение и предотвращает проблему исчезающих градиентов.

# - **Почему важна скорость обучения (learning rate)?**
#   Слишком большая скорость обучения может привести к нестабильному обучению, а слишком маленькая — к медленному прогрессу.

# === Итог ===
# Нейронная сеть — это мощный инструмент, который проходит несколько этапов от инициализации до обучения. 
# Каждый компонент (слои, активации, оптимизация) играет важную роль в достижении модели идеального состояния.
